[
  {
    "path": "posts/2022-02-25-corner-distributions-in-the-epl-part-1/",
    "title": "Corner Distributions in the EPL: Part 1",
    "description": "How can the number of home and away corners in the EPL be modelled, and can we build a prediction model?",
    "author": [
      {
        "name": "Nick Zani",
        "url": {}
      }
    ],
    "date": "2022-02-25",
    "categories": [],
    "contents": "\r\nIntroduction\r\nI recently got a copy of Joseph Buchdahl’s latest book: Monte Carlo Or Bust: Simple Simulations for Aspiring Sports Bettors. It has a great primer on probability theory, and combines this well with sports data. I decided to put it to the by test modelling the number of corners in EPL matches.\r\nThe first question is, why corners?\r\nThey are a ‘count’ variable so in theory should follow one of the standard distributions\r\nIt feels like they should be correlated with other variables, like shots, meaning there is the possibility of building a model to predict the number of corners in future matches based on past performance\r\nThey have possibly had less focus than goals which hopefully means there is a higher liklihood of finding an edge, however most bookmakers still provide markets and there is reasonable liquidity on Betfair\r\nData\r\nJoseph’s website, Football Data, contains a huge amount of historic football data, along with the odds. I started there and imported the data for the 2020/21 EPL season. I changed the format of this from wide to long, by creating two dataframes: one for home teams and one for away teams, and then stuck this together. Note for the home dataframe, corners conceded is given by the number of away corners (ac) and vice-versa.\r\n\r\n\r\nlibrary(tibble)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(readr)\r\nlibrary(tidyr)\r\nlibrary(janitor)\r\nlibrary(ggplot2)\r\n\r\nepl_2020_21 <- readr::read_csv('https://www.football-data.co.uk/mmz4281/2021/E0.csv') %>%\r\n  clean_names() %>%\r\n  mutate(date = as.Date(date, format=\"%d/%m/%Y\"),\r\n         start_hr = str_extract(time, \"[0-9]{2}\"))\r\n\r\nhome_data <- epl_2020_21 %>%\r\n  dplyr::select(date, home_team, ftr, fthg, hc, hs, hst, ac) %>%\r\n  mutate(team_type = \"Home\",\r\n         win_flag = case_when(ftr == \"H\" ~ 1, TRUE ~ 0),\r\n         draw_flag = case_when(ftr == \"D\" ~ 1, TRUE ~ 0)) %>%\r\n  rename(team = home_team,\r\n         full_time_goals = fthg,\r\n         corners = hc,\r\n         shots = hs,\r\n         shots_target = hst,\r\n         corners_conceded = ac)\r\n\r\naway_data <- epl_2020_21 %>%\r\n  dplyr::select(date, away_team, ftr, ftag, ac, as, ast, hc) %>%\r\n  mutate(team_type = \"Away\",\r\n         win_flag = case_when(ftr == \"A\" ~ 1, TRUE ~ 0),\r\n         draw_flag = case_when(ftr == \"D\" ~ 1, TRUE ~ 0)) %>%\r\n  rename(team = away_team,\r\n         full_time_goals = ftag,\r\n         corners = ac,\r\n         shots = as,\r\n         shots_target = ast,\r\n         corners_conceded = hc)\r\n\r\nlong_data = bind_rows(home_data, away_data)\r\n\r\n\r\n\r\nCorner Distribution\r\nLet’s start by graphing the corner distribution for home and away teams. We can use our long dataframe for this and pass the team type (home or away) into the fill aesthetic. Note the option of preseve = \"single\" to stop bars taking up the entire x axis if the other is zero. We can see different distributions for home and away teams, with the mean number of corners higher for home teams.\r\n\r\n\r\nlong_data %>%\r\n  ggplot(aes(x = corners, group = team_type, fill = team_type)) +\r\n  geom_bar(position = position_dodge(preserve = \"single\")) +\r\n  scale_fill_manual(values = c(\"orchid3\", \"skyblue1\")) +\r\n  theme_classic() +\r\n  labs(title = \"Corner Count by Team Type\", \r\n       subtitle = \"2020/21 EPL\\n\",\r\n       fill = \"Home/Away Team\",\r\n       x = \"Number of Corners\",\r\n       y = \"Count\\n\") +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18),\r\n        plot.subtitle = element_text(size = 14),\r\n        plot.caption = element_text(size = 12),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18)\r\n  )\r\n\r\n\r\n\r\n\r\nThis looks roughly like the a Poisson distribution. We know that for Poisson distributions the variance should be equal to the mean so let’s check that with some summary statistics:\r\n\r\n\r\nlong_data %>%\r\n  group_by(team_type) %>%\r\n  summarise(mean_corners = mean(corners),\r\n            var_corners = var(corners)) %>%\r\n  ungroup() %>%\r\n  mutate(mean_var_ratio = var_corners/mean_corners) %>%\r\n  mutate_if(is.numeric, round, 2)\r\n\r\n\r\n# A tibble: 2 x 4\r\n  team_type mean_corners var_corners mean_var_ratio\r\n  <chr>            <dbl>       <dbl>          <dbl>\r\n1 Away              4.63        7.11           1.54\r\n2 Home              5.56        9.24           1.66\r\n\r\nNot too bad, however the variance is 50 - 60% higher than the mean, giving us a ‘long tail’ on our distribution. This means our data might be better suited to a negative binomial distribution.\r\nThe goodfit function from the vcd package allows us to test how well our data fits both the Poisson and negative binomial distribution. From here on we will only consider home corners. We will create two dataframes with a fit for each, and then compare these to actual values.\r\n\r\n\r\nlibrary(vcd)\r\n\r\nhc_poisson <- goodfit(home_data$corners, type = \"poisson\", method = \"MinChisq\")\r\nhc_nbinomial <- goodfit(home_data$corners, type = \"nbinomial\", method = \"MinChisq\")\r\n\r\nhc_poisson_df <- data.frame(corners = hc_poisson$count,\r\n                            fitted = hc_poisson$fitted,\r\n                            type = \"Poisson Fit\")\r\n\r\nhc_nbinom_df <- data.frame(corners = hc_nbinomial$count,\r\n                            fitted = hc_nbinomial$fitted,\r\n                            type = \"Negative Binom Fit\")\r\nhc_actual <- home_data %>%\r\n  group_by(corners) %>%\r\n  count() %>%\r\n  ungroup() %>%\r\n  mutate(type = \"Actual\")\r\n\r\nfits_df <- bind_rows(hc_poisson_df, hc_nbinom_df)\r\n\r\n\r\n\r\nFor goodness of fit tests the null hypothesis that the data is consistent with a specified reference distribution. The summary of the fit gives us a p value which allows us to make a decision on this.\r\n\r\n\r\nsummary(hc_poisson)\r\n\r\n\r\n\r\n     Goodness-of-fit test for poisson distribution\r\n\r\n            X^2 df     P(> X^2)\r\nPearson 114.132 15 2.584289e-17\r\n\r\nHere we reject the null hypothesis for the Poisson distribution as the P value is very small, and well below the 0.05 threshold often used.\r\n\r\n\r\nsummary(hc_nbinomial)\r\n\r\n\r\n\r\n     Goodness-of-fit test for nbinomial distribution\r\n\r\n             X^2 df  P(> X^2)\r\nPearson 10.35587 14 0.7357304\r\n\r\nHere we cannot reject the null hypothesis for the negative binomial as the P value is >> 0.05, so we infer that the negative binomial is the correct choice for fitting our data. We can confirm this by plotting the fitted values compared to the observed ones:\r\n\r\n\r\nggplot() +\r\n  geom_col(data = hc_actual, aes(x = corners, y = n), fill = \"grey70\") +\r\n  geom_line(data = fits_df, aes(x = corners, y = fitted, colour = type), size = 2) +\r\n  scale_colour_brewer(type = \"qual\", palette = 2) +\r\n  theme_classic() +\r\n  labs(title = \"Actual and Fitted Corner Distribution\", \r\n       subtitle = \"Home Corners, 2020/21 EPL\\n\",\r\n       colour = \"Fit Type\",\r\n       x = \"Home Corners\",\r\n       y = \"Actual / Fitted Value\\n\",\r\n       caption = \"\\nNick Zani | @nickzani\") +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18),\r\n        plot.subtitle = element_text(size = 14),\r\n        plot.caption = element_text(size = 12),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18)\r\n  )\r\n\r\n\r\n\r\n\r\nBuilding a Predictive Corner Model\r\nNow we understand the distribution, we can take this a step further and see if we can build a predictive model for corners. The general theme of most betting tips seems to be ‘team x has had a lot of corners in their last y matches, which implies they are likely to have a high number in their next match.’ We are now in a position to test that theory.\r\nTo start with we will create a dataset for home corners, and include the last 3 matches the home team has played. We will then ‘lag’ the previous 3 matches to gives us features to use in the model. For example, how many corners has the home team taken in their previous three matches. Because there will be missing values for the first three games of the season we will omit these with the na.omit() function.\r\n\r\n\r\nhc_model_data <- home_data %>%\r\n  arrange(team, date) %>%\r\n  group_by(team) %>%\r\n  mutate(lag_1_home_corner = lag(corners, n = 1),\r\n         lag_2_home_corner = lag(corners, n = 2),\r\n         lag_3_home_corner = lag(corners, n = 3),\r\n         total_hc_last_3 = lag_1_home_corner + lag_2_home_corner + lag_3_home_corner) %>%\r\n  ungroup() %>%\r\n  na.omit()\r\n\r\n\r\n\r\nNow we have the data we can use the glm.nb from the MASS package to create a regression model. Note that loading the MASS package interferes with the dplyr selection function.\r\n\r\n\r\nlibrary(MASS)\r\nlibrary(broom)\r\noptions(scipen=999)\r\n\r\nm1_nb <- glm.nb(corners ~ lag_1_home_corner + lag_2_home_corner + lag_3_home_corner, \r\n                data = hc_model_data)\r\n\r\nbroom::tidy(m1_nb) %>%\r\n mutate_if(is.numeric, round, 3)\r\n\r\n\r\n# A tibble: 4 x 5\r\n  term              estimate std.error statistic p.value\r\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)          1.48      0.093    16.0     0    \r\n2 lag_1_home_corner    0.009     0.01      0.882   0.378\r\n3 lag_2_home_corner    0.022     0.01      2.23    0.026\r\n4 lag_3_home_corner    0.014     0.01      1.40    0.16 \r\n\r\nNote the p values for each of the features we created - only one meets the threshold for significance, and that is the number of corners from two games ago. Also note the estimates are all very low.\r\nPossibly we can add in additional features to improve the predictive power, for example, say a team was very attacking and unlucky in winning corners. This should show up in other variables, like shots. Let’s expand our model with more variables, including this time lagged home goals, home shots and also corners conceded.\r\n\r\n\r\nhc_model_data2 <- home_data %>%\r\n  arrange(team, date) %>%\r\n  group_by(team) %>%\r\n  mutate(lag_1_home_corner = lag(corners, n = 1),\r\n         lag_2_home_corner = lag(corners, n = 2),\r\n         lag_3_home_corner = lag(corners, n = 3),\r\n         lag_1_home_shots = lag(shots, n = 1),\r\n         lag_2_home_shots = lag(shots, n = 2),\r\n         lag_3_home_shots = lag(shots, n = 3),\r\n         lag_1_home_goals = lag(full_time_goals, n = 1),\r\n         lag_2_home_goals = lag(full_time_goals, n = 2),\r\n         lag_3_home_goals = lag(full_time_goals, n = 3),\r\n         lag_1_corners_conceded = lag(corners_conceded, n = 1),\r\n         lag_2_corners_conceded = lag(corners_conceded, n = 2),\r\n         lag_3_corners_conceded = lag(corners_conceded, n = 3)) %>%\r\n  ungroup() %>%\r\n  na.omit()\r\n\r\nm1_nb2 <- glm.nb(corners ~ lag_1_home_corner + lag_2_home_corner + lag_3_home_corner +\r\n                          lag_1_home_shots + lag_2_home_shots + lag_3_home_shots +\r\n                          lag_1_home_goals + lag_2_home_goals + lag_3_home_goals +\r\n                          lag_1_corners_conceded + lag_2_corners_conceded + lag_3_corners_conceded, \r\n                data = hc_model_data2)\r\n\r\nbroom::tidy(m1_nb2) %>%\r\n mutate_if(is.numeric, round, 3)\r\n\r\n\r\n# A tibble: 13 x 5\r\n   term                   estimate std.error statistic p.value\r\n   <chr>                     <dbl>     <dbl>     <dbl>   <dbl>\r\n 1 (Intercept)               1.56      0.187     8.35    0    \r\n 2 lag_1_home_corner         0.007     0.013     0.547   0.585\r\n 3 lag_2_home_corner         0.015     0.013     1.17    0.241\r\n 4 lag_3_home_corner         0         0.012    -0.025   0.98 \r\n 5 lag_1_home_shots          0         0.007     0.057   0.954\r\n 6 lag_2_home_shots          0.001     0.007     0.188   0.851\r\n 7 lag_3_home_shots          0.009     0.007     1.32    0.188\r\n 8 lag_1_home_goals         -0.015     0.026    -0.561   0.575\r\n 9 lag_2_home_goals          0.038     0.024     1.56    0.118\r\n10 lag_3_home_goals         -0.003     0.024    -0.129   0.897\r\n11 lag_1_corners_conceded    0.001     0.012     0.045   0.964\r\n12 lag_2_corners_conceded   -0.015     0.012    -1.24    0.216\r\n13 lag_3_corners_conceded   -0.013     0.013    -1.05    0.293\r\n\r\nNote the p values now - none are significant. This tells us that there is very little predictive power in previous home performances on predicting the number of future corners. Some of the variable estimates are also negative, for example the number of goals scored in the previous game. This can be interpreted as every additional goal in the previous game should reduce the number of corners we would expect in the next game by 0.015.\r\nConclusions and Next Steps\r\nWe’ve seen that corners can be modelled using a negative binomial distribution, and that it is very difficult to use simple past performance to predict the future numbers of corners. There are a number of possible reasons for this:\r\nThere is a large amount of randomness in how teams win corners\r\nThe data we have does not contain enough ‘signal’. E.g. we need more information about pitch size, number of attackers playing etc.\r\nThere is a non linear relationship we are missing here. For example, where a team is winning easily they are less likely to attack, so will possibly win fewer corners\r\nEither way, if you encounter a ‘tipster’ who posts ‘research’ like ‘team x have scored y corners in their last 3 matches’ then be very afraid. There could well be a non linear relationship that can be teased out with decision trees, however looking at past form is in no way going to give this.\r\nStay tuned for the next steps:\r\nIncrease the amount of data with current and previous seasons. This should hopefully smooth the distributions out (it wasn’t a perfect negative binomial)\r\nUse some machine learning algorithms to check if we can find some non linear relationships between the features we have created\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-25-corner-distributions-in-the-epl-part-1/corner-distributions-in-the-epl-part-1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-03-02T15:26:24+00:00",
    "input_file": "corner-distributions-in-the-epl-part-1.knit.md"
  },
  {
    "path": "posts/2022-01-25-showing-averages-within-large-datsets/",
    "title": "Showing Averages Within Large Datsets",
    "description": "Using the geomborderline package to make averages jump out.",
    "author": [
      {
        "name": "Nick Zani",
        "url": {}
      }
    ],
    "date": "2022-01-25",
    "categories": [],
    "contents": "\r\nIntroduction\r\nLast week I used the ggborderline package to tidy up line graphs. This week I used the same data again to try and show how the average has changed over time, while still showing the overall distribution.\r\nData\r\nI chose 15 states from the Tidy Tuesday data on the number of nurses employed in each US state by month. I then created three more data frames. One showing the max and min values by year, one showing the average, and one for the final labels.\r\n\r\n\r\nlibrary(tibble)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(readr)\r\nlibrary(tidyr)\r\nlibrary(janitor)\r\nlibrary(ggplot2)\r\n\r\nnurses <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-05/nurses.csv') %>%\r\n  clean_names()\r\n\r\nstates <- nurses %>%\r\n  filter(year == min(year)) %>%\r\n  arrange(total_employed_rn) %>%\r\n  head(30) %>%\r\n  tail(15) %>%\r\n  pull(state)\r\n\r\nnurses_filtered <- nurses %>%\r\n  filter(state %in% states)\r\n\r\nmax_min <- nurses_filtered %>%\r\n  group_by(year) %>%\r\n  summarise(min_employed = min(total_employed_rn, na.rm = TRUE),\r\n            max_employed = max(total_employed_rn, na.rm = TRUE))\r\n\r\naverages <- nurses_filtered %>%\r\n  group_by(year) %>%\r\n  summarise(mean_employed = mean(total_employed_rn, na.rm = TRUE),\r\n            median_employed = median(total_employed_rn, na.rm = TRUE))\r\n\r\nlabels <- averages %>%\r\n  ungroup() %>%\r\n  filter(year == max(year)) %>%\r\n  mutate(label = \"Median\")\r\n\r\n\r\n\r\nBuilding Up the Plot\r\nThe plot is made up of four parts: - The ribbon between the maximum and minimum points, with the colour defined in the fill option - The line of each individual state - The average line, passed to geom_borderline so it jumps out against the other lines - The secondary axis for labelling\r\n\r\n\r\nlibrary(ggborderline)\r\n\r\nggplot(max_min, aes(year)) +\r\n  geom_ribbon(aes(ymin = min_employed - 200, ymax = max_employed + 200), fill = \"grey95\") +\r\n  geom_line(data = nurses_filtered, aes(x = year, y = total_employed_rn, group = state), colour = \"grey 75\", size = 1) +\r\n  geom_borderline(data = averages, aes(x = year, y = median_employed), size = 2, bordersize = 2, colour = \"royalblue4\") +\r\n  theme_classic() +\r\n  scale_y_continuous(labels = scales::comma_format(),\r\n                     sec.axis = dup_axis(breaks = labels$median_employed,\r\n                                         labels = labels$label,\r\n                                         name = \"\")) +\r\n  scale_x_continuous(expand = expansion(mult = c(0.05, 0.01))) +\r\n  labs(title = \"Nurse Numbers in Selected States\", \r\n       subtitle = \"geom_ribbon + geom_borderline\\n\",\r\n       x = \"\",\r\n       y = \"Count of Nurses\\n\",\r\n       caption = \"\\nNick Zani | @nickzani\") +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14, colour = \"grey 15\"),\r\n        axis.text.x = element_text(size = 18),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18, colour = \"grey 15\"),\r\n        plot.subtitle = element_text(size = 14, colour = \"grey 15\"),\r\n        plot.caption = element_text(size = 12, colour = \"grey 15\"),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18),\r\n        legend.position = \"none\",\r\n        axis.line.y.right = element_line(color = \"white\"),\r\n        axis.ticks.y.right = element_line(color = \"white\"),\r\n        axis.text.y.right = element_text(color = \"royalblue4\")\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-25-showing-averages-within-large-datsets/showing-averages-within-large-datsets_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-01-25T19:17:44+00:00",
    "input_file": "showing-averages-within-large-datsets.knit.md"
  },
  {
    "path": "posts/2022-01-17-labelling-line-graphs/",
    "title": "Tidying up Line Graphs",
    "description": "Using ggborderline and secondary axis labels.",
    "author": [
      {
        "name": "Nick Zani",
        "url": {}
      }
    ],
    "date": "2022-01-17",
    "categories": [],
    "contents": "\r\nIntroduction\r\nLast week I had a play around with the geomtextpath package, and this week I wanted to expand on that with ggborderline, again using some Tidy Tuesday data from October 2021 on the number of nurse numbers in different American states.\r\nData\r\nNo data manipulation to be done here, just import and filter the data for some selected states. I chose states which had similar numbers so that some of the lines in the plots would cross over later.\r\n\r\n\r\nlibrary(tibble)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(readr)\r\nlibrary(tidyr)\r\nlibrary(janitor)\r\nlibrary(ggplot2)\r\n\r\nnurses <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-05/nurses.csv')\r\n\r\n\r\nnurses_filtered <- nurses %>%\r\n  clean_names() %>%\r\n  filter(state %in% c(\"Alabama\", \"Arizona\", \"Oregon\", \"Iowa\"))\r\n\r\n\r\n\r\nInitial Visualisation\r\nThe off-the-shelf ggplot looks fine, but there are definitely some improvements we can make.\r\n\r\n\r\nnurses_filtered %>%\r\n  ggplot(aes(x = year, y = total_employed_rn, group = state, colour = state)) +\r\n  geom_line(size = 2) +\r\n  scale_colour_brewer(type = \"qual\", palette = 2) +\r\n  theme_classic() +\r\n  scale_y_continuous(labels = scales::comma_format()) +\r\n  labs(title = \"Nurse Numbers in Selected States\", \r\n       subtitle = \"Standard ggplot\\n\",\r\n       colour = \"State\",\r\n       x = \"\",\r\n       y = \"Count of Nurses\\n\",\r\n       caption = \"\\nNick Zani | @nickzani\") +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18),\r\n        plot.subtitle = element_text(size = 14),\r\n        plot.caption = element_text(size = 12),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18)\r\n  )\r\n\r\n\r\n\r\n\r\nUsing ggborderline\r\nThe ggborderline is a great package for tidying up line graphs where the lines overlap. It adds in a white border to the lines which can be tweaked in thickness\r\n\r\n\r\nlibrary(ggborderline)\r\n\r\nnurses_filtered %>%\r\n  ggplot(aes(x = year, y = total_employed_rn, group = state, colour = state)) +\r\n  geom_borderline(size = 2, bordersize = 0.75) +\r\n  scale_colour_brewer(type = \"qual\", palette = 2) +\r\n  theme_classic() +\r\n  scale_y_continuous(labels = scales::comma_format()) +\r\n  labs(title = \"Nurse Numbers in Selected States\", \r\n       subtitle = \"With ggborderline\\n\",\r\n       colour = \"State\",\r\n       x = \"\",\r\n       y = \"Count of Nurses\\n\",\r\n       caption = \"\\nNick Zani | @nickzani\") +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18),\r\n        plot.subtitle = element_text(size = 14),\r\n        plot.caption = element_text(size = 12),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18)\r\n  )\r\n\r\n\r\n\r\n\r\nImproving the Labels\r\nThe final tweak is to fix the labels. The legend is not in order, and makes the graph harder to read. By adding a secondary axis in, we can simplify this. The trick is to create a second dataframe which contains just the labels at the maximum values.\r\n\r\n\r\nlibrary(ggborderline)\r\n\r\nmax_y_value <- nurses_filtered %>%\r\n  filter(year == max(year))\r\n\r\nnurses_filtered %>%\r\n  ggplot(aes(x = year, \r\n             y = total_employed_rn, \r\n             group = state, \r\n             colour = state)) +\r\n  geom_borderline(size = 2, bordersize = 0.75) +\r\n  scale_colour_brewer(type = \"qual\", palette = 2) +\r\n  theme_classic() +\r\n  scale_y_continuous(labels = scales::comma_format(),\r\n                     sec.axis = dup_axis(breaks = max_y_value$total_employed_rn,\r\n                                         labels = max_y_value$state,\r\n                                         name = \"\")) +\r\n  labs(title = \"Nurse Numbers in Selected States\", \r\n       subtitle = \"With ggborderline + secondary axis labelling\\n\",\r\n       colour = \"State\",\r\n       x = \"\",\r\n       y = \"Count of Nurses\\n\",\r\n       caption = \"\\nNick Zani | @nickzani\") +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18),\r\n        plot.subtitle = element_text(size = 14),\r\n        plot.caption = element_text(size = 12),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18),\r\n        legend.position = \"none\",\r\n        axis.line.y.right = element_line(color = \"white\"),\r\n        axis.ticks.y.right = element_line(color = \"white\")\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-17-labelling-line-graphs/labelling-line-graphs_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-01-18T09:50:15+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-12-tidy-tuesday-week-2-2022/",
    "title": "Tidy Tuesday: Week 2 2022",
    "description": "Testing Out the geomtextpath Package.",
    "author": [
      {
        "name": "Nick Zani",
        "url": {}
      }
    ],
    "date": "2022-01-12",
    "categories": [],
    "contents": "\r\nIntroduction\r\nI’ve wanted to try out the geomtextpath package for a while so thought I would use this week’s Tidy Tuesday data on bees to give it a go.\r\n\r\nData\r\nNot a huge amount of work required to tidy this up, just a few tweaks to dates and percentages\r\n\r\n\r\nlibrary(tibble)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(readr)\r\nlibrary(tidyr)\r\nlibrary(janitor)\r\nlibrary(ggplot2)\r\n\r\ncolony <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/colony.csv')\r\nstressor <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/stressor.csv')\r\n\r\nstressor_clean <- stressor %>% \r\n  mutate(stress_pct = stress_pct/100) %>%\r\n  mutate(month_num = case_when(months == \"January-March\" ~ 1,\r\n                               months == \"April-June\" ~ 4,\r\n                               months == \"July-September\" ~ 7,\r\n                               months == \"October-December\" ~ 10,)) %>%\r\n  mutate(year_mon = as.Date(paste0(year, \"-\", str_pad(month_num, 2, side = \"left\", pad = 0), \"-01\")),\r\n         formatted_dt = format(year_mon, \"%b-%y\"))\r\n\r\n\r\n\r\nVisualisation\r\nDownload the package from Github:\r\n\r\n\r\nlibrary(remotes)\r\nremotes::install_github(\"AllanCameron/geomtextpath\")\r\n\r\n\r\n\r\nPlot the graph:\r\n\r\n\r\nlibrary(geomtextpath)\r\n\r\nstressor_clean %>%\r\n  filter(state == \"Texas\") %>%\r\n  filter(stressor != \"Unknown\") %>%\r\n  filter(stressor != \"Other\") %>%\r\n  ggplot(aes(x = year_mon, y = stress_pct, colour = stressor)) +\r\n    geom_point(alpha = 0.3,\r\n               size = 2) +\r\n    geom_textsmooth(aes(label = stressor, colour = stressor),\r\n                    method = \"loess\", \r\n                    formula = y ~ x,\r\n                    size = 5, \r\n                    linetype = 3, \r\n                    fontface = 2, \r\n                    linewidth = 1.5)+\r\n  scale_colour_brewer(type = \"qual\", palette = 2) +\r\n  theme_classic() +\r\n  scale_y_continuous(labels = scales::percent_format(accuracy=1)) +\r\n  labs(title = \"Stressor Impact on Texas Bee Colonies\", \r\n       subtitle = \"Unknown and Other Stressors Removed\\n\",\r\n       x = \"\",\r\n       y = \"Percent of Colonies Affected\\n\",\r\n       caption = \"\\nNick Zani | @nickzani\") +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18),\r\n        plot.subtitle = element_text(size = 14),\r\n        plot.caption = element_text(size = 12),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18),\r\n        legend.position = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-12-tidy-tuesday-week-2-2022/tidy-tuesday-week-2-2022_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-01-12T11:15:19+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-07-tidy-tuesday-week-1-2022/",
    "title": "Tidy Tuesday: Week 1 2022",
    "description": "Bring your own data: When are the twins waking up?",
    "author": [
      {
        "name": "Nick Zani",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-01-07",
    "categories": [],
    "contents": "\r\nIntroduction\r\nThe theme for Tidy Tuesday week 1 was bring your own data. For a while I have had a feeling that our just-turned-two year old twins have been waking up earlier and earlier, so I decided to use my Apple watch step data to try to prove this.\r\n\r\nData\r\nThe Apple health data can be exported via the health app as an xml file. By filtering on the step count and Apple watch flag we can extract the date and time using str_match. We will assume that if there were steps in the hours between midnight and 5am it was because of (at least one of) the twins waking up.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(readr)\r\nlibrary(tibble)\r\nlibrary(stringr)\r\nlibrary(ggplot2)\r\nlibrary(scales)\r\n\r\nsteps <- as_tibble(read_lines(file = \"../../_data/apple_health_extract.xml\")) %>%\r\n  filter(str_detect(value, 'StepCount')) %>%\r\n  filter(str_detect(value, 'Apple Watch')) %>%\r\n  mutate(creation_dt = as.Date(str_match(value, 'creationDate=\\\\\"\\\\\"(\\\\d+-\\\\d+-\\\\d+) \\\\d+:\\\\d+:\\\\d+')[,2]),\r\n         creation_hr = as.numeric(str_match(value, 'creationDate=\\\\\"\\\\\"\\\\d+-\\\\d+-\\\\d+ (\\\\d+):\\\\d+:\\\\d+')[,2])\r\n         )\r\n\r\n\r\n\r\nNow we will filter for the last few months, extract the day of the week and group the wakeup times. Finally we will merge on the counts by month so we can get a percentage, to take account of the fact that different months are different length, and on some occasions my watch was charging.\r\n\r\n\r\naggregated_steps <- steps %>%\r\n  filter(creation_dt >= as.Date(\"2021-04-01\") & creation_dt <= as.Date(\"2021-12-31\")) %>%\r\n  select(-value) %>%\r\n  distinct() %>%\r\n  mutate(weekday_name = strftime(creation_dt,'%a'),\r\n         weekday_num = as.numeric(strftime(creation_dt,'%u')),\r\n         month_name = strftime(creation_dt,'%b'),\r\n         month_num = as.numeric(strftime(creation_dt,'%m'))\r\n         ) %>%\r\n  mutate(grouping = case_when(creation_hr %in% c(0,1) ~ \"0 - 1am\",\r\n                              creation_hr %in% c(2,3) ~ \"2 - 3am\",\r\n                              creation_hr %in% c(4,5) ~ \"4 - 5am\"\r\n                              )\r\n         ) %>%\r\n  filter(!is.na(grouping)) %>%\r\n  group_by(month_name, month_num, grouping) %>%\r\n  summarise(cnt = n()) %>%\r\n  ungroup()\r\n\r\nmonthly_cnts <- aggregated_steps %>%\r\n  group_by(month_name, month_num) %>%\r\n  summarise(mthly_total = sum(cnt))\r\n\r\npercentages <- aggregated_steps %>%\r\n  inner_join(monthly_cnts) %>%\r\n  mutate(percent_wakeups = cnt/mthly_total)\r\n\r\n\r\n\r\nVisualisation\r\nNow we have the data in tidy form we can pass to ggplot to visualise. This confirms my theory that the twins have been waking up earlier and earlier, with a big change in November, where over half the wakeups were before 4am!\r\n\r\n\r\npercentages %>%\r\n  ggplot(aes(x = reorder(month_name, month_num),\r\n             y = percent_wakeups,\r\n             fill = grouping)\r\n         ) +\r\n  geom_col() +\r\n  theme_void() +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  labs(title = \"When Are the Twins Waking Up?\", \r\n       subtitle = \"Percentage of Wakeups by Time, by Month\\n\",\r\n       x = \"\",\r\n       y = \"\",\r\n       fill = \"Wakeup Time\",\r\n       caption = \"\\nNick Zani | @nickzani\") +\r\n  scale_fill_brewer(palette = 3) +\r\n  theme(strip.text = element_text(size = 18),\r\n        axis.title = element_text(size = 14),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.text.y = element_text(size = 18),\r\n        plot.title = element_text(size = 18),\r\n        plot.subtitle = element_text(size = 14),\r\n        plot.caption = element_text(size = 12),\r\n        legend.text = element_text(size = 16),\r\n        legend.title = element_text(size = 18)\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-07-tidy-tuesday-week-1-2022/tidy-tuesday-week-1-2022_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-01-12T10:29:45+00:00",
    "input_file": {}
  }
]
